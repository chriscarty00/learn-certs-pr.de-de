### YamlMime:Course
title: Data Engineering on Microsoft Azure
metadata:
  title: 'Course DP-203T00-A: Data Engineering on Microsoft Azure'
  description: 'Course DP-203T00-A: Data Engineering on Microsoft Azure'
uid: course.dp-203t00
courseNumber: 'DP-203T00-A'
hoursToComplete: 96
iconUrl: /media/learn/certification/course.svg
skillsGained:
- skill: Informieren Sie sich über Rechen- und Speicheroptionen für Data Engineering-Workloads in Azure
- skill: Führen Sie interaktive Abfragen mit serverlosen SQL-Pools aus 
- skill: Führen Sie die Datenexploration und -transformation in Azure Databricks durch
- skill: Erforschen, Umwandeln und Laden von Daten in das Data Warehouse mit Apache Spark
- skill: Daten aufnehmen und in das Daten-Warehouse laden
- skill: Transformieren Sie Daten mit Azure Data Factory- oder Azure Synapse-Pipelines
- skill: Integrieren Sie Daten von Notebooks in Azure Data Factory- oder Azure Synapse-Pipelines
- skill: Unterstützung der Hybrid-Transaktionsanalysebearbeitung (HTAP) mit Azure Synapse Link
- skill: Führen Sie mit Azure Synapse Analytics End-to-End-Sicherheit durch
- skill: Führen Sie eine Echtzeit-Stream-Verarbeitung mit Stream Analytics durch
- skill: Erstellen Sie eine Stream-Verarbeitungslösung mit Event Hubs und Azure Databricks 
learningPartnersLink: /learn/certifications/partners
locales:
- en
levels:
- intermediate
roles:
- data-engineer
products:
- azure
exams:
- uid: exam.dp-203
summary: |-
  In diesem Kurs lernen die Teilnehmer das Data Engineering in Bezug auf die Arbeit mit Batch- und Echtzeit-Analyselösungen unter Verwendung von Azure-Datenplattformtechnologien kennen. Die Teilnehmer beginnen mit Grundlagen der wichtigsten Computer- und Speichertechnologien, die zum Erstellen einer analytischen Lösung verwendet werden. Die Teilnehmer lernen, wie man Daten, die in Dateien in einem Datenmeer gespeichert sind, interaktiv untersuchen kann. Sie lernen die verschiedenen Ingestions-Techniken kennen, die zum Laden von Daten mit der Apache Spark-Funktion in Azure Synapse Analytics oder Azure Databricks verwendet werden können, oder wie man mithilfe von Azure Data Factory oder Azure Synapse Pipelines ingestiert. Die Teilnehmer lernen auch die verschiedenen Möglichkeiten kennen, Daten mit denselben Technologien zu transformieren, mit denen sie aufgenommen werden. Sie werden verstehen, wie wichtig es ist, Sicherheit zu implementieren, um sicherzustellen, dass die Daten in Ruhe oder während des Transports geschützt sind. Anschließend wird gezeigt, wie man ein Echtzeit-Analysesystem zur Erstellung von Echtzeit-Analyselösungen erstellt.

  #### Zielgruppenprofil
  Die primäre Zielgruppe für diesen Kurs sind Datenfachleute, Datenarchitekten und Experten für Geschäftsintelligenz, die mehr über Daten-Engineering und das Erstellen von Analyselösungen mit Hilfe von Datenplattformtechnologien in Microsoft Azure erfahren möchten. Die sekundäre Zielgruppe für diesen Kurs sind Datenanalysten und Datenwissenschaftler, die mit auf Microsoft Azure basierenden Analyselösungen arbeiten.
prerequisitesSection: |-
  Erfolgreiche Teilnehmer beginnen diesen Kurs mit Kenntnissen in Cloud Computing und Kerndatenkonzepten sowie Berufserfahrung mit Datenlösungen. 
  
  Speziell abschließen&#58;
  
  - AZ-900 - Azure Fundamentals
  - DP-900 - Microsoft Azure Data Fundamentals
outlineSection: |-
  ### Modul 1&#58; Untersuchen von Rechen- und Speicheroptionen für Daten-Engineering-Workloads
  Dieses Modul bietet einen Überblick über die Optionen der Azure-Computer- und Speichertechnologie, die Dateningenieuren zur Verfügung stehen, die analytische Workloads erstellen.In diesem Modul erfahren Sie, wie man das Datenmeer strukturiert und die Dateien für Explorations-, Streaming- und Batch-Workloads optimieren kann.Die Teilnehmer lernen, wie man das Datenmeer in Ebenen der Datenverfeinerung organisiert, während Dateien durch Stapel- und Stream-Verarbeitung transformiert werden. Anschließend lernen sie, wie man Indizes für ihre Datensets wie CSV-, JSON- und Parkettdateien erstellen und diese für eine mögliche Beschleunigung von Abfragen und Workloads verwenden kann.
  #### Lektionen
  - Einführung in Azure Synapse Analytics
  - Beschreiben von Azure Databricks
  - Einführung in den Azure Datenmeer-Speicher 
  - Die Architektur des Datenmeeres beschreiben
  - Arbeiten mit Datenströmen, mit hilfe von Azure Stream Analytics

  #### Lab&#58; Erkunden Sie die Rechen- und Speicheroptionen für Daten-Engineering-Workloads
  * Kombination von Streaming und Stapelverarbeitung mit einer einzigen Pipeline
  * Organisieren des Datenmeeres in Ebenen der Dateitransformation
  * Indizierung des Datenmeer-Speichers zur Beschleunigung von Abfragen und Workloads

  Nach Abschluss dieses Moduls sind die Teilnehmer in der Lage&#58;
  - Azure Synapse Analytics zu beschreiben
  - Azure Databricks zu beschreiben
  - Azure Datenmeer-Speichers zu beschreiben
  - die Architektur des Datenmeeres zu beschreiben
  - Azure Stream Analytics zu beschreiben 
    
  ### Modul 2&#58; Führen Sie interaktive Abfragen mit serverlosen SQL-Pools von Azure Synapse Analytics aus
  In diesem Modul lernen die Teilnehmer anhand von T-SQL-Anweisungen, die von einem serverlosen SQL-Pool in Azure Synapse Analytics ausgeführt werden, wie man mit Dateien arbeitet, die im Datenmeer und in externen Dateiquellen gespeichert sind. Die Teilnehmer fragen Parkettdateien ab, die in einem Datenmeer gespeichert sind, sowie CSV-Dateien, die in einem externen Datenspeicher gespeichert sind. Als Nächstes erstellen sie Azure Active Directory-Sicherheitsgruppen und erzwingen den Zugriff auf Dateien im Datenmeer über die rollenbasierte Zugriffssteuerung (RBAC) und Zugriffssteuerungslisten (ACLs).
  #### Lektionen
  - Erkunden Sie die Funktionen der serverlosen SQL-Poolsvon AzureSynapse
  - Abfragen vonDaten im Meer mit Hilfevon serverlosen SQL-Pools von Azure Synapse 
  - Erstellen Sie Metadatenobjekte in serverlosen SQL-Pools von Azure Synapse
  - Sichern Sie Daten und verwalten Sie Benutzer in serverlosen SQL-Poolsvon Azure Synapse

  #### Lab&#58; Führen Sie interaktive Abfragen mit serverlosen SQL-Pools aus
  - Abfragenvon Parkettdaten mit serverlosen SQL-Pools 
  - Erstellen externer Tabellen für Parkett- und CSV-Dateien 
  - Erstellen von Ansichten mitserverlosen SQL-Pools
  - Zugriff auf Daten in einem Datensee bei Verwendung von serverlosen SQL-Pools sichern
  - Konfigurieren von Datenmeer-Sicherheit mit Hilfe rollenbasierter Zugriffssteuerung (RBAC) und der Zugriffssteuerungsliste

  Nach Abschluss dieses Moduls sind die Teilnehmer in der Lage&#58;
  - Funktionen der serverlosen SQL-Poolsvon AzureSynapse zu verstehen
  - Daten im Datenmeer mit Hilfevon serverlosen SQL-Pools von Azure Synapse abzufragen 
  - Metadatenobjekte inserverlosen SQL-Poolsvon Azure Synapse zu erstellen 
  - Daten zu sichern und Benutzer inserverlosen SQL-Poolsvon Azure Synapse zu verwalten 

  ### Modul 3: Datenexploration und -transformation in Azure Databricks

  Dieses Modul lehrt die Verwendung verschiedener Apache Spark DataFrame-Methoden zur Untersuchung und Transformation von Daten in Azure Databricks. Die Teilnehmer lernen, wie sie Standard-DataFrame-Methoden zur Erkundung und Umwandlung von Daten durchführen können. Sie lernen auch, wie man erweiterte Aufgaben ausführen, z. B. doppelte Daten entfernen, Datums- / Zeitwerte bearbeiten, Spalten umbenennen und Daten aggregieren kann.

  #### Lektionen  

  * Beschreiben von Azure Databricks

  * Lesen und Schreiben von Daten in Azure Databricks

  * Arbeiten mit DataFrames in Azure Databricks

  * Arbeiten mit erweiterten Methoden von DataFrames in Azure Databricks


  ### Übung: Datenuntersuchung und -umwandlung in Azure Databricks

  ####
  * DataFrames in Azure Databricks verwenden, um Daten zu untersuchen und zu filtern
  * Einen DataFrame für schnellere nachfolgende Abfragen zwischenspeichern
  * Entfernen von doppelten Daten
  * Manipulation von Datums-/Zeitwerten
  * Entfernen und Umbenennen von DataFrame-Spalten
  * Aggregieren von Daten, die in einem DataFrame gespeichert sind

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Azure Databricks zu beschreiben

  * Lesen und Schreiben von Daten in Azure Databricks

  * Arbeiten mit DataFrames in Azure Databricks

  * Mit erweiterten Methoden von DataFrames in Azure Databricks zu arbeiten.


  ### Module 4: Erforschen, Transformieren und Laden von Daten in das Data Warehouse mit Apache SparkIn diesem 

  Modul lernen Sie, wie Sie in einem Data Lake gespeicherte Daten untersuchen, die Daten transformieren und in einen relationalen Datenspeicher laden. Die Teilnehmer werden Parkett- und JSON-Dateien untersuchen  und Techniken verwenden, um JSON-Dateien mit hierarchischen Strukturen abzufragen und zu transformieren. Anschließend werden die Teilnehmer Apache Spark verwenden, um Daten in das Data Warehouse zu laden und Parquet-Daten im Datenmeer mit Daten im dedizierten SQL-Pool zu verbinden.

  #### Lektionen

  * Verstehen von Big Data Engineering mit Apache Spark in Azure Synapse Analytics

  * Daten mit Apache Spark-Notebooks in Azure Synapse Analytics einlesen

  * Daten mit DataFrames in Apache Spark Pools in Azure Synapse Analytics zu transformieren

  * Integration von SQL- und Apache Spark-Pools in Azure Synapse Analytics

  #### Übung: Erforschen, Transformieren und Laden von Daten in das Data Warehouse mit Apache Spark

  ####

  * Datenexploration in Synapse Studio durchführen
  * Daten mit Spark-Notebooks in Azure Synapse Analytics einlesen
  * Transformieren von Daten mit DataFrames in Spark-Pools in Azure Synapse Analytics
  * Integrieren von SQL- und Spark-Pools in Azure Synapse Analytics

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Big Data Engineering mit Apache Spark in Azure Synapse Analytics zu beschreiben

  * Daten mit Apache Spark-Notebooks in Azure Synapse Analytics einlesen

  * Daten mit DataFrames in Apache Spark-Pools in Azure Synapse Analytics transformieren

  * Integration von SQL- und Apache Spark-Pools in Azure Synapse Analytics

  ### Modul 5: Einlesen und Laden von Daten in das Data Warehouse

  In diesem Modul lernen die Teilnehmer, wie sie Daten mit Hilfe von T-SQL-Skripten und Synapse Analytics-Integrationspipelines in das Data Warehouse einspeisen. Die Teilnehmer lernen, wie sie Daten mit PolyBase und COPY unter Verwendung von T-SQL in Synapse-spezifische SQL-Pools laden können. Die Teilnehmer lernen auch, wie man das Workload-Management zusammen mit einer Kopieraktivität in einer Azure Synapse-Pipeline für die Datenaufnahme im Petabyte-Bereich verwendet.

  #### Lektionen

  * Verwendung von bewährten Praktiken für das Laden von Daten in Azure Synapse Analytics

  * Ingestion im Petabyte-Bereich mit Azure Data Factory

  #### Übung : Einlesen und Laden von Daten in das Data Warehouse

  ####

  * Ingestion im Petabyte-Bereich mit Azure Synapse Pipelines durchführen
  * Daten mit PolyBase und COPY mit T-SQL importieren
  * Verwendung von bewährten Praktiken zum Laden von Daten in Azure Synapse Analytics

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Bewährte Praktiken zum Laden von Daten in Azure Synapse Analytics anwenden

  * Ingestion im Petabyte-Bereich mit Azure Data Factory

  ### Module 6: Datenumwandlung mit Azure Data Factory oder Azure Synapse Pipelines

  In diesem Modul lernen die Teilnehmer, wie sie Datenintegrationspipelines erstellen, um Daten aus mehreren Datenquellen aufzunehmen, Daten mithilfe von Mapping-Datenflüssen zu transformieren und Daten in eine oder mehrere Datensenken zu verschieben.

  #### Lektionen

  * Datenintegration mit Azure Data Factory oder Azure Synapse Pipelines

  * Codefreie Transformation in großem Maßstab mit Azure Data Factory oder Azure Synapse Pipelines

  #### Übung: Daten transformieren mit Azure Data Factory oder Azure Synapse Pipelines

  ####

  * Codefreie Transformationen in großem Maßstab mit Azure Synapse Pipelines ausführen
  * Erstellen einer Datenpipeline zum Importieren schlecht formatierter CSV-Dateien
  * Mapping Data Flows erstellen

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Datenintegration mit Azure Data Factory durchzuführen

  * Code-freie Transformationen in großem Umfang mit Azure Data Factorydurchzuführen.

  ### Modul 7: Orchestrierung von Datenbewegungen und -transformationen in Azure Synapse Pipelines

  In diesem Modul lernen Sie, wie Sie verknüpfte Dienste erstellen und Datenbewegungen und -transformationen mithilfe von Notebooks in Azure Synapse Pipelines orchestrieren können.

  #### Lektionen

  * Orchestrierung der Datenbewegung und -umwandlung in Azure Data Factory

  #### Übung : Orchestrierung der Datenbewegung und -umwandlung in Azure Synapse Pipelines

  ####
  * Integrieren von Daten aus Notebooks mit Azure Data Factory oder Azure Synapse Pipelines

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Datenbewegung und -transformation in Azure Synapse Pipelines zu orchestrieren.

  ### Modul 8: End-to-End-Sicherheit mit Azure Synapse Analytics

  In diesem Modul lernen die Teilnehmer, wie sie einen Synapse Analytics-Arbeitsbereich und seine unterstützende Infrastruktur sichern. Die Teilnehmer werden den SQL Active Directory Admin beobachten, IP-Firewall-Regeln verwalten, Geheimnisse mit Azure Key Vault verwalten und über einen mit Key Vault verknüpften Service und Pipeline-Aktivitäten auf diese Geheimnisse zugreifen. Die Teilnehmer verstehen, wie man Sicherheit auf Spaltenebene, Sicherheit auf Zeilenebene und dynamische Datenmaskierung bei der Verwendung von dedizierten SQL-Pools implementiert.

  #### Lektionen

  * Sichern eines Data Warehouse in Azure Synapse Analytics

  * Konfigurieren und Verwalten von Geheimnissen in Azure Key Vault

  *  Implementierung von Compliance-Kontrollen für sensible Daten

  #### Übung : End-to-End-Sicherheit mit Azure Synapse Analytics

  ####

  * Sichern der Azure Synapse Analyticsunterstützenden Infrastruktur
  * Sichern Sie den Azure Synapse Analytics-Arbeitsbereich und die verwalteten Dienste.
  * Sichern der Daten im Azure Synapse Analytics-Arbeitsbereich

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Sichern Sie ein Data Warehouse in Azure Synapse Analytics.

  * Konfigurieren und Verwalten von Geheimnissen in Azure Key Vault

  * Implementierung von Compliance-Kontrollen für sensible Daten

  ### Modul 9: Hybrid Transactional Analytical Processing HTAP) mit Azure Synapse Link unterstützen

  In diesem Modul lernen die Teilnehmer, wie Azure Synapse Link eine nahtlose Verbindung zwischen einem Azure Cosmos DB-Konto und einem Synapse-Arbeitsbereich ermöglicht. Die Teilnehmer lernen zu verstehen , wie man die Synapse-Verknüpfung aktiviert und konfiguriert und wie der Azure Cosmos DB-Analysespeicher mit Apache Spark und serverlosem SQL abgefragt  wird.

  #### Lektionen

  * Entwicklung hybrider transaktionaler und analytischer Verarbeitung mit Azure Synapse Analytics

  * Konfigurieren von Azure Synapse Link mit Azure Cosmos DB

  * Abfrage von Azure Cosmos DB mit Apache Spark-Pools

  * Abfrage von Azure Cosmos DB mit serverlosen SQL-Pools

  #### Übung: Unterstützung der Hybrid Transactional Analytical Processing (HTAP) mit Azure Synapse Link

  ####
  * Azure Synapse Link mit Azure Cosmos DB konfigurieren
  * Abfrage von Azure Cosmos DB mit Apache Spark für Synapse Analytics
  * Abfrage von Azure Cosmos DB mit serverlosem SQL-Pool für Azure Synapse Analytics

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Hybride transaktionale und analytische Verarbeitung mit Azure Synapse Analytics zu entwerfen

  * Konfigurieren von Azure Synapse Link mit Azure Cosmos DB

  * Abfrage vonAzure Cosmos DB mit Apache Spark für Azure Synapse Analytics

  * Abfrage von Azure Cosmos DB mit SQL Serverless für Azure Synapse Analytics


  ### Modul 10: Stream-Verarbeitung in Echtzeit mit Stream Analytics

  In diesem Modul lernen die Teilnehmer, wie sie Streaming-Daten mit Azure Stream Analytics verarbeiten können. Die Teilnehmer nehmen Fahrzeugtelemetriedaten in Event Hubs auf und verarbeiten diese Daten dann in Echtzeit mit Hilfe verschiedener Fensterfunktionen in Azure Stream Analytics. Sie werden die Daten an Azure Synapse Analytics ausgeben. Schließlich lernen die Teilnehmer, wie sie den Stream Analytics-Auftrag skalieren können, um den Durchsatz zu erhöhen.

  #### Lektionen

  * Zuverlässiges Messaging für Big Data-Anwendungen mithilfe von Azure Event Hubs aktivieren

  * Arbeiten mit Datenströmen mithilfe von Azure Stream Analytics

  * Einlesen von Datenströmen mit Azure Stream Analytics

  ####Übung: Stream-Verarbeitung in Echtzeit mit Stream Analytics

  ####
  * Verwendung von Stream Analytics zur Verarbeitung von Echtzeitdaten aus Event Hubs
  * Verwendung von Stream Analytics-Fensterfunktionen zur Erstellung von Aggregaten und zur Ausgabe an Synapse Analytics
  * Den Azure Stream Analytics-Job skalieren, um den Durchsatz durch Partitionierung zu erhöhen
  * Repartitionierung der Stream-Eingabe zur Optimierung der Parallelisierung

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Zuverlässiges Messaging für Big Data-Anwendungen mithilfe von Azure Event Hubs zu ermöglichen

  * mit Azure Stream Analytics mit Datenströmen zu arbeiten

  * Datenströme mit Azure Stream Analytics einlesen

  ### Modul 11: Erstellen einer Stream Processing-Lösung mit Event Hubs und Azure Databricks

  In diesem Modul lernen die Teilnehmer, wie sie mit Event Hubs und Spark Structured Streaming in Azure Databricks Streaming-Daten in großem Umfang aufnehmen und verarbeiten können. Die Teilnehmer lernen die wichtigsten Funktionen und Anwendungen von Structured Streaming kennen. Die Teilnehmer implementieren Schiebefenster, um Datenblöcke zu aggregieren und wenden Wasserzeichen an, um veraltete Daten zu entfernen. Schließlich werden die Teilnehmer eine Verbindung zu Event Hubs herstellen, um Streams zu lesen und zu schreiben.

  #### Lektionen

  * Verarbeiten von Streaming-Daten mit Azure Databricks structured streaming


  #### Übung : Erstellen einer Stream-Verarbeitungslösung mit Event Hubs und Azure Databricks

  ####
  * Die wichtigsten Funktionen und Verwendungszwecke von Structured Streaming erkunden
  * Daten aus einer Datei streamen und in ein verteiltes Dateisystem schreiben
  * Verwendung von Schiebefenstern, um Datenpakete und nicht alle Daten zu aggregieren
  * Wasserzeichen anwenden, um veraltete Daten zu entfernen
  * Verbindung zu Event Hubs Lese- und Schreibstreams

  Nach Abschluss dieses Moduls werden die Teilnehmer in der Lage sein:

  * Verarbeiten von Streaming-Daten mit Azure Databricks structured Streaming